{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json_lines\n",
    "import logging\n",
    "import collections\n",
    "import json\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, OrderedDict, defaultdict as ddict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_squad(path):\n",
    "\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    data_dict = {'question': [], 'context': [], 'id': [], 'answer': []}\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                if len(qa['answers']) == 0:\n",
    "                    data_dict['question'].append(question)\n",
    "                    data_dict['context'].append(context)\n",
    "                    data_dict['id'].append(qa['id'])\n",
    "                else:\n",
    "                    for answer in  qa['answers']:\n",
    "                        data_dict['question'].append(question)\n",
    "                        data_dict['context'].append(context)\n",
    "                        data_dict['id'].append(qa['id'])\n",
    "                        data_dict['answer'].append(answer)\n",
    "    id_map = ddict(list)\n",
    "    for idx, qid in enumerate(data_dict['id']):\n",
    "        id_map[qid].append(idx)\n",
    "\n",
    "    data_dict_collapsed = {'question': [], 'context': [], 'id': []}\n",
    "    if data_dict['answer']:\n",
    "        data_dict_collapsed['answer'] = []\n",
    "    for qid in id_map:\n",
    "        ex_ids = id_map[qid]\n",
    "        data_dict_collapsed['question'].append(data_dict['question'][ex_ids[0]])\n",
    "        data_dict_collapsed['context'].append(data_dict['context'][ex_ids[0]])\n",
    "        data_dict_collapsed['id'].append(qid)\n",
    "        if data_dict['answer']:\n",
    "            all_answers = [data_dict['answer'][idx] for idx in ex_ids]\n",
    "            data_dict_collapsed['answer'].append({'answer_start': [answer['answer_start'] for answer in all_answers],\n",
    "                                                  'text': [answer['text'] for answer in all_answers]})\n",
    "    return data_dict_collapsed\n",
    "##################################################################\n",
    "def read_squad_examples(input_file, debug=False):\n",
    "    # Read data\n",
    "#     unproc_data = []\n",
    "#     with gzip.open(input_file, 'rt', encoding='utf-8') as f:  # opening file in binary(rb) mode\n",
    "#         for item in json_lines.reader(f):\n",
    "#             # print(item) #or use print(item['X']) for printing specific data\n",
    "#             unproc_data.append(item)\n",
    "\n",
    "    # Delete header\n",
    "#     unproc_data = unproc_data[1:]\n",
    "#     if debug:\n",
    "#         unproc_data = unproc_data[:100]\n",
    "    \n",
    "#     print(unproc_data)\n",
    "    with open(input_file, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "        unproc_data = squad_dict['data']\n",
    "        \n",
    "    ###################### Make Examples ######################\n",
    "    examples = []\n",
    "    for item in unproc_data:\n",
    "        for passage in item['paragraphs']:\n",
    "            # 1. Get Context\n",
    "            doc_tokens = []\n",
    "            for token in item['context_tokens']:\n",
    "                # BERT has only [SEP] in it's word piece vocabulary. because we keps all separators char length 5\n",
    "                # we can replace all of them with [SEP] without modifying the offset\n",
    "                if token[0] in ['[TLE]', '[PAR]', '[DOC]']:\n",
    "                    token[0] = '[SEP]'\n",
    "                doc_tokens.append(token[0])\n",
    "\n",
    "            # 2. qas\n",
    "            for qa in item['qas']:\n",
    "                qas_id = qa['qid']\n",
    "                question_text = qa['question']\n",
    "\n",
    "                answer_lst = []  # Check for duplicate question\n",
    "                for answer in qa['detected_answers']:\n",
    "                    orig_answer_text = answer['text']\n",
    "                    # We could find so many duplicate \"Detected Answer\"...It needs to be erased\n",
    "                    if orig_answer_text in answer_lst:\n",
    "                        continue\n",
    "                    else:\n",
    "                        answer_lst.append(orig_answer_text)\n",
    "\n",
    "                    # Only take the first span\n",
    "                    start_position = answer['token_spans'][0][0]\n",
    "                    end_position = answer['token_spans'][0][1]\n",
    "\n",
    "                    example = SquadExample(\n",
    "                        qas_id=qas_id,\n",
    "                        question_text=question_text,\n",
    "                        doc_tokens=doc_tokens,\n",
    "                        orig_answer_text=orig_answer_text,\n",
    "                        start_position=start_position,\n",
    "                        end_position=end_position)\n",
    "                    examples.append(example)\n",
    "    return examples\n",
    "\n",
    "class SquadExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for the Squad dataset.\n",
    "    For examples without an answer, the start and end position are -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 question_text,\n",
    "#                  doc_tokens,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "#         self.doc_tokens = doc_tokens\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"qas_id: %s\" % (self.qas_id)\n",
    "        s += \", question_text: %s\" % (self.question_text)\n",
    "#         s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "        if self.start_position:\n",
    "            s += \", start_position: %d\" % (self.start_position)\n",
    "        if self.end_position:\n",
    "            s += \", end_position: %d\" % (self.end_position)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zhanj289/cs224n_robust_qa/adv_model/datasets/train/newsqa', 'rb') as f:\n",
    "        squad_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'paragraphs'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dict['data'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'Editor\\'s note: Campbell Brown anchors CNN\\'s \"Campbell Brown: No Bias, No Bull\" at 8 p.m. ET Mondays through Fridays. She delivered this commentary during the \"Cutting through the Bull\" segment of Tuesday night\\'s broadcast.\\n\\nCNN\\'s Campbell Brown says \"having no life\" isn\\'t a requirement for a man to get a job.\\n\\n(CNN) -- How many times have politicians been warned about the dangers of an open microphone? And yet, on Tuesday, the lectern mic at the National Governors Conference picked up this little nugget from Pennsylvania\\'s Democratic Gov. Ed Rendell.\\n\\nHe\\'s having a conversation near the lectern about President-elect Barack Obama\\'s choice for to lead the Homeland Security Department, Arizona Gov. Janet Napolitano. Here is what Rendell said about Napolitano:\\n\\nRendell: Janet\\'s perfect for that job. Because for that job, you have to have no life. Janet has no family. Perfect. She can devote, literally, 19-20 hours a day to it\\n\\nWow. Now, I\\'m sure Gov. Napolitano has many qualifications for the job beyond having no family, and therefore the ability to devote 20 hours a day to the job.  Watch Campbell Brown\\'s commentary Â»\\n\\nBut it is fascinating to me that that is the quality being highlighted here as so perfect. C\\'mon. Homeland Security Secretary Michael Chertoff is married with two grown children. His predecessor, Tom Ridge, had a family. Anybody remember a debate about whether they would have trouble balancing the demands of work and family?\\n\\nNow, I am a fan of Gov. Rendell. He has been on this show many times. I like him for his candor. In our attempts to cut through the bull, he delivers far less bull than most politicians. But it is his frankness here that raises so many questions.\\n\\n1. If a man had been Obama\\'s choice for the job, would having a family or not having a family ever even have been an issue? Would it have ever prompted a comment? Probably not. We all know the assumption tends to be that with a man, there is almost always a wife in the wings managing those family concerns.\\n\\n2. As a woman, hearing this, it is hard not to wonder if we are counted out for certain jobs, certain opportunities, because we do have a family or because we are in our child-bearing years. Are we? It is a fair question.\\n\\n3. If you are a childless, single woman with suspicions that you get stuck working holidays, weekends and the more burdensome shifts more often than your colleagues with families, are those suspicions well-founded? Probably so. Is there an assumption that if you\\'re family-free then you have no life? By some, yes.\\n\\nAgain Gov. Rendell, I don\\'t mean to rake you over the coals. I know what you meant to say. But your comments do perpetuate stereotypes that put us in boxes, both mothers and single women.\\n\\nIn government and beyond, men have been given the benefit of the doubt when it comes to striking the right work-life balance. Women are owed the same consideration.\\n\\nThe opinions expressed in this commentary are solely those of Campbell Brown.',\n",
       "  'qas': [{'question': 'What types of issues should men and women be treated equally on, according to the reading?',\n",
       "    'id': '00893b02555542268ddc3e09e0454b1e',\n",
       "    'answers': [{'answer_start': 2855, 'text': 'work-life balance.'}]},\n",
       "   {'question': 'Women deserve what?',\n",
       "    'id': '6d762f5953ad4bc78f89b8ae3c6858e0',\n",
       "    'answers': [{'answer_start': 2889, 'text': 'the same consideration.'}]},\n",
       "   {'question': 'Who has many qualifications according to Brown?',\n",
       "    'id': '49ffad4fcd9449a086911f5ce2726974',\n",
       "    'answers': [{'answer_start': 956, 'text': 'Gov. Napolitano'}]},\n",
       "   {'question': 'Rendell is a good choice because she has what?',\n",
       "    'id': '68c2e0862b764532831c69337e6c3b83',\n",
       "    'answers': [{'answer_start': 865, 'text': 'no family.'}]}]}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dict['data'][190]['paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "unproc_data = []\n",
    "with gzip.open('/home/zhanj289/cs224n_robust_qa/adv_model/data/train/NewsQA.jsonl.gz', 'rt', encoding='utf-8') as f:  # opening file in binary(rb) mode\n",
    "    for item in json_lines.reader(f):\n",
    "        # print(item) #or use print(item['X']) for printing specific data\n",
    "        unproc_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'WASHINGTON (CNN) -- One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\\n\\nThe Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\\n\\nSgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\\n\\nAt a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank\\'s younger sister, Mary Pero.\\n\\nStrank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\\n\\nStrank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\\n\\nJonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\\n\\nHe hailed Strank as \"a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\"',\n",
       " 'qas': [{'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#0',\n",
       "   'question': 'What war was the Iwo Jima battle a part of?',\n",
       "   'answers': ['II'],\n",
       "   'qid': 'eb767219b657430d803c28f31da0f1d2',\n",
       "   'question_tokens': [['What', 0],\n",
       "    ['war', 5],\n",
       "    ['was', 9],\n",
       "    ['the', 13],\n",
       "    ['Iwo', 17],\n",
       "    ['Jima', 21],\n",
       "    ['battle', 26],\n",
       "    ['a', 33],\n",
       "    ['part', 35],\n",
       "    ['of', 40],\n",
       "    ['?', 42]],\n",
       "   'detected_answers': [{'text': 'II',\n",
       "     'char_spans': [[67, 68]],\n",
       "     'token_spans': [[15, 15]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#1',\n",
       "   'question': 'Where was Michael Strank born?',\n",
       "   'answers': ['Czechoslovakia'],\n",
       "   'qid': 'f0d948a6226b49dc8963151aeb51a2a7',\n",
       "   'question_tokens': [['Where', 0],\n",
       "    ['was', 6],\n",
       "    ['Michael', 10],\n",
       "    ['Strank', 18],\n",
       "    ['born', 25],\n",
       "    ['?', 29]],\n",
       "   'detected_answers': [{'text': 'Czechoslovakia',\n",
       "     'char_spans': [[326, 339]],\n",
       "     'token_spans': [[63, 63]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#2',\n",
       "   'question': 'Where was STrank killed?',\n",
       "   'answers': ['on the island'],\n",
       "   'qid': '5de4d7fe4356447997704bfad418f2ed',\n",
       "   'question_tokens': [['Where', 0],\n",
       "    ['was', 6],\n",
       "    ['STrank', 10],\n",
       "    ['killed', 17],\n",
       "    ['?', 23]],\n",
       "   'detected_answers': [{'text': 'on the island',\n",
       "     'char_spans': [[983, 995]],\n",
       "     'token_spans': [[178, 180]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#3',\n",
       "   'question': 'Who was among six who famously raised flag on Iwo Jima?',\n",
       "   'answers': ['Sgt. Michael Strank,'],\n",
       "   'qid': '91e9e24d19a243d4a97d149ffa84c70c',\n",
       "   'question_tokens': [['Who', 0],\n",
       "    ['was', 4],\n",
       "    ['among', 8],\n",
       "    ['six', 14],\n",
       "    ['who', 18],\n",
       "    ['famously', 22],\n",
       "    ['raised', 31],\n",
       "    ['flag', 38],\n",
       "    ['on', 43],\n",
       "    ['Iwo', 46],\n",
       "    ['Jima', 50],\n",
       "    ['?', 54]],\n",
       "   'detected_answers': [{'text': 'Sgt. Michael Strank,',\n",
       "     'char_spans': [[289, 308]],\n",
       "     'token_spans': [[54, 58]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#4',\n",
       "   'question': 'What rank did Michael Strank hold?',\n",
       "   'answers': ['Sgt.'],\n",
       "   'qid': 'f216cb9f43744e228978091774604ff2',\n",
       "   'question_tokens': [['What', 0],\n",
       "    ['rank', 5],\n",
       "    ['did', 10],\n",
       "    ['Michael', 14],\n",
       "    ['Strank', 22],\n",
       "    ['hold', 29],\n",
       "    ['?', 33]],\n",
       "   'detected_answers': [{'text': 'Sgt.',\n",
       "     'char_spans': [[289, 292]],\n",
       "     'token_spans': [[54, 55]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#5',\n",
       "   'question': 'When was the certificate given to his sister?',\n",
       "   'answers': ['Tuesday.'],\n",
       "   'qid': '089e09ffd2e440538c5fe55a6fbf1a34',\n",
       "   'question_tokens': [['When', 0],\n",
       "    ['was', 5],\n",
       "    ['the', 9],\n",
       "    ['certificate', 13],\n",
       "    ['given', 25],\n",
       "    ['to', 31],\n",
       "    ['his', 34],\n",
       "    ['sister', 38],\n",
       "    ['?', 44]],\n",
       "   'detected_answers': [{'text': 'Tuesday.',\n",
       "     'char_spans': [[177, 184]],\n",
       "     'token_spans': [[33, 34]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#6',\n",
       "   'question': 'Who was killed on Iwo Jima in World War II ?',\n",
       "   'answers': ['Michael Strank,'],\n",
       "   'qid': 'bad075856db843648efb7d0447666b9f',\n",
       "   'question_tokens': [['Who', 0],\n",
       "    ['was', 4],\n",
       "    ['killed', 8],\n",
       "    ['on', 15],\n",
       "    ['Iwo', 18],\n",
       "    ['Jima', 22],\n",
       "    ['in', 27],\n",
       "    ['World', 30],\n",
       "    ['War', 36],\n",
       "    ['II', 40],\n",
       "    ['?', 43]],\n",
       "   'detected_answers': [{'text': 'Michael Strank,',\n",
       "     'char_spans': [[294, 308]],\n",
       "     'token_spans': [[56, 58]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#7',\n",
       "   'question': 'WHERE WAS Strank, born?',\n",
       "   'answers': ['Czechoslovakia'],\n",
       "   'qid': 'd98952cebd6a4a55b74cdef4e64d2527',\n",
       "   'question_tokens': [['WHERE', 0],\n",
       "    ['WAS', 6],\n",
       "    ['Strank', 10],\n",
       "    [',', 16],\n",
       "    ['born', 18],\n",
       "    ['?', 22]],\n",
       "   'detected_answers': [{'text': 'Czechoslovakia',\n",
       "     'char_spans': [[326, 339]],\n",
       "     'token_spans': [[63, 63]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#8',\n",
       "   'question': 'What did Strank not receive?',\n",
       "   'answers': ['citizenship papers.'],\n",
       "   'qid': '112ea4764a82474d99982e8ddfe56dfd',\n",
       "   'question_tokens': [['What', 0],\n",
       "    ['did', 5],\n",
       "    ['Strank', 9],\n",
       "    ['not', 16],\n",
       "    ['receive', 20],\n",
       "    ['?', 27]],\n",
       "   'detected_answers': [{'text': 'citizenship papers.',\n",
       "     'char_spans': [[551, 569]],\n",
       "     'token_spans': [[100, 102]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#9',\n",
       "   'question': 'Where was Strank born?',\n",
       "   'answers': ['Czechoslovakia'],\n",
       "   'qid': '821a22757f7a4e498fd20050e4cc8a4f',\n",
       "   'question_tokens': [['Where', 0],\n",
       "    ['was', 6],\n",
       "    ['Strank', 10],\n",
       "    ['born', 17],\n",
       "    ['?', 21]],\n",
       "   'detected_answers': [{'text': 'Czechoslovakia',\n",
       "     'char_spans': [[326, 339]],\n",
       "     'token_spans': [[63, 63]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#10',\n",
       "   'question': 'Who received the certificate?',\n",
       "   'answers': [\"Strank's younger sister, Mary Pero.\"],\n",
       "   'qid': '3944899575e2496e9b49bc6040ce5290',\n",
       "   'question_tokens': [['Who', 0],\n",
       "    ['received', 4],\n",
       "    ['the', 13],\n",
       "    ['certificate', 17],\n",
       "    ['?', 28]],\n",
       "   'detected_answers': [{'text': \"Strank's younger sister, Mary Pero.\",\n",
       "     'char_spans': [[730, 764]],\n",
       "     'token_spans': [[132, 139]]}]},\n",
       "  {'id': './cnn/stories/644a3f79470d3b457efacc7d4ea33577d59e69c1.story#11',\n",
       "   'question': 'Who was among the six who raised the flag in Iwo Jima?',\n",
       "   'answers': ['Sgt. Michael Strank,'],\n",
       "   'qid': '0507f1c29219435d9d0ae30e9a855308',\n",
       "   'question_tokens': [['Who', 0],\n",
       "    ['was', 4],\n",
       "    ['among', 8],\n",
       "    ['the', 14],\n",
       "    ['six', 18],\n",
       "    ['who', 22],\n",
       "    ['raised', 26],\n",
       "    ['the', 33],\n",
       "    ['flag', 37],\n",
       "    ['in', 42],\n",
       "    ['Iwo', 45],\n",
       "    ['Jima', 49],\n",
       "    ['?', 53]],\n",
       "   'detected_answers': [{'text': 'Sgt. Michael Strank,',\n",
       "     'char_spans': [[289, 308]],\n",
       "     'token_spans': [[54, 58]]}]}],\n",
       " 'context_tokens': [['WASHINGTON', 0],\n",
       "  ['(', 11],\n",
       "  ['CNN', 12],\n",
       "  [')', 15],\n",
       "  ['--', 17],\n",
       "  ['One', 20],\n",
       "  ['of', 24],\n",
       "  ['the', 27],\n",
       "  ['Marines', 31],\n",
       "  ['shown', 39],\n",
       "  ['in', 45],\n",
       "  ['a', 48],\n",
       "  ['famous', 50],\n",
       "  ['World', 57],\n",
       "  ['War', 63],\n",
       "  ['II', 67],\n",
       "  ['photograph', 70],\n",
       "  ['raising', 81],\n",
       "  ['the', 89],\n",
       "  ['U.S.', 93],\n",
       "  ['flag', 98],\n",
       "  ['on', 103],\n",
       "  ['Iwo', 106],\n",
       "  ['Jima', 110],\n",
       "  ['was', 115],\n",
       "  ['posthumously', 119],\n",
       "  ['awarded', 132],\n",
       "  ['a', 140],\n",
       "  ['certificate', 142],\n",
       "  ['of', 154],\n",
       "  ['U.S.', 157],\n",
       "  ['citizenship', 162],\n",
       "  ['on', 174],\n",
       "  ['Tuesday', 177],\n",
       "  ['.', 184],\n",
       "  ['The', 187],\n",
       "  ['Marine', 191],\n",
       "  ['Corps', 198],\n",
       "  ['War', 204],\n",
       "  ['Memorial', 208],\n",
       "  ['in', 217],\n",
       "  ['Virginia', 220],\n",
       "  ['depicts', 229],\n",
       "  ['Strank', 237],\n",
       "  ['and', 244],\n",
       "  ['five', 248],\n",
       "  ['others', 253],\n",
       "  ['raising', 260],\n",
       "  ['a', 268],\n",
       "  ['flag', 270],\n",
       "  ['on', 275],\n",
       "  ['Iwo', 278],\n",
       "  ['Jima', 282],\n",
       "  ['.', 286],\n",
       "  ['Sgt', 289],\n",
       "  ['.', 292],\n",
       "  ['Michael', 294],\n",
       "  ['Strank', 302],\n",
       "  [',', 308],\n",
       "  ['who', 310],\n",
       "  ['was', 314],\n",
       "  ['born', 318],\n",
       "  ['in', 323],\n",
       "  ['Czechoslovakia', 326],\n",
       "  ['and', 341],\n",
       "  ['came', 345],\n",
       "  ['to', 350],\n",
       "  ['the', 353],\n",
       "  ['United', 357],\n",
       "  ['States', 364],\n",
       "  ['when', 371],\n",
       "  ['he', 376],\n",
       "  ['was', 379],\n",
       "  ['3', 383],\n",
       "  [',', 384],\n",
       "  ['derived', 386],\n",
       "  ['U.S.', 394],\n",
       "  ['citizenship', 399],\n",
       "  ['when', 411],\n",
       "  ['his', 416],\n",
       "  ['father', 420],\n",
       "  ['was', 427],\n",
       "  ['naturalized', 431],\n",
       "  ['in', 443],\n",
       "  ['1935', 446],\n",
       "  ['.', 450],\n",
       "  ['However', 452],\n",
       "  [',', 459],\n",
       "  ['U.S.', 461],\n",
       "  ['Citizenship', 466],\n",
       "  ['and', 478],\n",
       "  ['Immigration', 482],\n",
       "  ['Services', 494],\n",
       "  ['recently', 503],\n",
       "  ['discovered', 512],\n",
       "  ['that', 523],\n",
       "  ['Strank', 528],\n",
       "  ['never', 535],\n",
       "  ['was', 541],\n",
       "  ['given', 545],\n",
       "  ['citizenship', 551],\n",
       "  ['papers', 563],\n",
       "  ['.', 569],\n",
       "  ['At', 572],\n",
       "  ['a', 575],\n",
       "  ['ceremony', 577],\n",
       "  ['Tuesday', 586],\n",
       "  ['at', 594],\n",
       "  ['the', 597],\n",
       "  ['Marine', 601],\n",
       "  ['Corps', 608],\n",
       "  ['Memorial', 614],\n",
       "  ['--', 623],\n",
       "  ['which', 626],\n",
       "  ['depicts', 632],\n",
       "  ['the', 640],\n",
       "  ['flag', 644],\n",
       "  ['-', 648],\n",
       "  ['raising', 649],\n",
       "  ['--', 657],\n",
       "  ['in', 660],\n",
       "  ['Arlington', 663],\n",
       "  [',', 672],\n",
       "  ['Virginia', 674],\n",
       "  [',', 682],\n",
       "  ['a', 684],\n",
       "  ['certificate', 686],\n",
       "  ['of', 698],\n",
       "  ['citizenship', 701],\n",
       "  ['was', 713],\n",
       "  ['presented', 717],\n",
       "  ['to', 727],\n",
       "  ['Strank', 730],\n",
       "  [\"'s\", 736],\n",
       "  ['younger', 739],\n",
       "  ['sister', 747],\n",
       "  [',', 753],\n",
       "  ['Mary', 755],\n",
       "  ['Pero', 760],\n",
       "  ['.', 764],\n",
       "  ['Strank', 767],\n",
       "  ['and', 774],\n",
       "  ['five', 778],\n",
       "  ['other', 783],\n",
       "  ['men', 789],\n",
       "  ['became', 793],\n",
       "  ['national', 800],\n",
       "  ['icons', 809],\n",
       "  ['when', 815],\n",
       "  ['an', 820],\n",
       "  ['Associated', 823],\n",
       "  ['Press', 834],\n",
       "  ['photographer', 840],\n",
       "  ['captured', 853],\n",
       "  ['the', 862],\n",
       "  ['image', 866],\n",
       "  ['of', 872],\n",
       "  ['them', 875],\n",
       "  ['planting', 880],\n",
       "  ['an', 889],\n",
       "  ['American', 892],\n",
       "  ['flag', 901],\n",
       "  ['on', 906],\n",
       "  ['top', 909],\n",
       "  ['of', 913],\n",
       "  ['Mount', 916],\n",
       "  ['Suribachi', 922],\n",
       "  ['on', 932],\n",
       "  ['February', 935],\n",
       "  ['23', 944],\n",
       "  [',', 946],\n",
       "  ['1945', 948],\n",
       "  ['.', 952],\n",
       "  ['Strank', 955],\n",
       "  ['was', 962],\n",
       "  ['killed', 966],\n",
       "  ['in', 973],\n",
       "  ['action', 976],\n",
       "  ['on', 983],\n",
       "  ['the', 986],\n",
       "  ['island', 990],\n",
       "  ['on', 997],\n",
       "  ['March', 1000],\n",
       "  ['1', 1006],\n",
       "  [',', 1007],\n",
       "  ['1945', 1009],\n",
       "  [',', 1013],\n",
       "  ['less', 1015],\n",
       "  ['than', 1020],\n",
       "  ['a', 1025],\n",
       "  ['month', 1027],\n",
       "  ['before', 1033],\n",
       "  ['the', 1040],\n",
       "  ['battle', 1044],\n",
       "  ['between', 1051],\n",
       "  ['Japanese', 1059],\n",
       "  ['and', 1068],\n",
       "  ['U.S.', 1072],\n",
       "  ['forces', 1077],\n",
       "  ['there', 1084],\n",
       "  ['ended', 1090],\n",
       "  ['.', 1095],\n",
       "  ['Jonathan', 1098],\n",
       "  ['Scharfen', 1107],\n",
       "  [',', 1115],\n",
       "  ['the', 1117],\n",
       "  ['acting', 1121],\n",
       "  ['director', 1128],\n",
       "  ['of', 1137],\n",
       "  ['CIS', 1140],\n",
       "  [',', 1143],\n",
       "  ['presented', 1145],\n",
       "  ['the', 1155],\n",
       "  ['citizenship', 1159],\n",
       "  ['certificate', 1171],\n",
       "  ['Tuesday', 1183],\n",
       "  ['.', 1190],\n",
       "  ['He', 1193],\n",
       "  ['hailed', 1196],\n",
       "  ['Strank', 1203],\n",
       "  ['as', 1210],\n",
       "  ['\"', 1213],\n",
       "  ['a', 1214],\n",
       "  ['true', 1216],\n",
       "  ['American', 1221],\n",
       "  ['hero', 1230],\n",
       "  ['and', 1235],\n",
       "  ['a', 1239],\n",
       "  ['wonderful', 1241],\n",
       "  ['example', 1251],\n",
       "  ['of', 1259],\n",
       "  ['the', 1262],\n",
       "  ['remarkable', 1266],\n",
       "  ['contribution', 1277],\n",
       "  ['and', 1290],\n",
       "  ['sacrifices', 1294],\n",
       "  ['that', 1305],\n",
       "  ['immigrants', 1310],\n",
       "  ['have', 1321],\n",
       "  ['made', 1326],\n",
       "  ['to', 1331],\n",
       "  ['our', 1334],\n",
       "  ['great', 1338],\n",
       "  ['republic', 1344],\n",
       "  ['throughout', 1353],\n",
       "  ['its', 1364],\n",
       "  ['history', 1368],\n",
       "  ['.', 1375],\n",
       "  ['\"', 1376]]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unproc_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_squad('/home/zhanj289/cs224n_robust_qa/adv_model/datasets/train/race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_squad_examples('/home/zhanj289/cs224n_robust_qa/adv_model/data/train/NewsQA.jsonl.gz', debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a ='dsasadasdsadas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'das'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'race'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'race'[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    document_id = path[-4:]\n",
    "#     path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    data_dict = {'question': [], 'context': [], 'id': [], 'answer': [], 'doc': []}\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                if len(qa['answers']) == 0:\n",
    "                    data_dict['question'].append(question)\n",
    "                    data_dict['context'].append(context)\n",
    "                    data_dict['id'].append(qa['id'])\n",
    "                    ###\n",
    "                    data_dict['doc'].append(document_id)\n",
    "                else:\n",
    "                    for answer in  qa['answers']:\n",
    "                        data_dict['question'].append(question)\n",
    "                        data_dict['context'].append(context)\n",
    "                        data_dict['id'].append(qa['id'])\n",
    "                        data_dict['answer'].append(answer)\n",
    "                        ###\n",
    "                        data_dict['doc'].append(document_id)\n",
    "                        \n",
    "    id_map = ddict(list)\n",
    "    \n",
    "    for idx, qid in enumerate(data_dict['id']):\n",
    "        id_map[qid].append(idx)\n",
    "#     print(len(id_map))\n",
    "#     print(id_map)\n",
    "    \n",
    "    data_dict_collapsed = {'question': [], 'context': [], 'id': [], 'doc': []}\n",
    "    if data_dict['answer']:\n",
    "        data_dict_collapsed['answer'] = []\n",
    "    for qid in id_map:\n",
    "        ex_ids = id_map[qid]\n",
    "        \n",
    "#         print(qid)\n",
    "#         print(ex_ids)\n",
    "        data_dict_collapsed['question'].append(data_dict['question'][ex_ids[0]])\n",
    "        data_dict_collapsed['context'].append(data_dict['context'][ex_ids[0]])\n",
    "        data_dict_collapsed['id'].append(qid)\n",
    "        #\n",
    "        data_dict_collapsed['doc'].append(data_dict['doc'][0])\n",
    "        if data_dict['answer']:\n",
    "            all_answers = [data_dict['answer'][idx] for idx in ex_ids]\n",
    "            data_dict_collapsed['answer'].append({'answer_start': [answer['answer_start'] for answer in all_answers],\n",
    "                                                  'text': [answer['text'] for answer in all_answers]})\n",
    "            \n",
    "    return data_dict_collapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict= read_squad('/home/zhanj289/cs224n_robust_qa/datasets/indomain_train/nat_questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_dict['doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def prepare_train_data(dataset_dict, tokenizer):\n",
    "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
    "                                   dataset_dict['context'],\n",
    "                                   truncation=\"only_second\",\n",
    "                                   stride=128,\n",
    "                                   max_length=384,\n",
    "                                   return_overflowing_tokens=True,\n",
    "                                   return_offsets_mapping=True,\n",
    "                                   padding='max_length')\n",
    "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "    \n",
    "    print(tokenized_examples.keys())\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    tokenized_examples['id'] = []\n",
    "    \n",
    "    inaccurate = 0\n",
    "    for i, offsets in enumerate(tqdm(offset_mapping)):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = dataset_dict['answer'][sample_index]\n",
    "        # Start/end character index of the answer in the text.\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = start_char + len(answer['text'][0])\n",
    "        tokenized_examples['id'].append(dataset_dict['id'][sample_index])\n",
    "        # Start token index of the current span in the text.\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        # End token index of the current span in the text.\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "            # assertion to check if this checks out\n",
    "            context = dataset_dict['context'][sample_index]\n",
    "            offset_st = offsets[tokenized_examples['start_positions'][-1]][0]\n",
    "            offset_en = offsets[tokenized_examples['end_positions'][-1]][1]\n",
    "            if context[offset_st : offset_en] != answer['text'][0]:\n",
    "                inaccurate += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    doc_map = {v: k for k,v in enumerate(set(dataset_dict['doc']))}\n",
    "    tokenized_examples['labels']  = [doc_map[ele] for ele in dataset_dict['doc'] ]\n",
    "\n",
    "#     print(len(tokenized_examples))  #7           \n",
    "#     print(len(tokenized_examples['start_positions']))  \n",
    "#     print(tokenized_examples['start_positions'][1:10])  \n",
    "#     print(len(tokenized_examples['end_positions']))\n",
    "#     print(len(tokenized_examples['labels'] ))\n",
    "#     print(tokenized_examples['labels'][1:10])\n",
    "    \n",
    "    total = len(tokenized_examples['id'])\n",
    "    print(f\"Preprocessing not completely accurate for {inaccurate}/{total} instances\")\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:00<00:00, 22618.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n",
      "Preprocessing not completely accurate for 0/196 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_examples = prepare_train_data(dataset_dict, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions', 'id', 'labels'])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_examples['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= (1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-f340045ef062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "a, b,c, d = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robustqa",
   "language": "python",
   "name": "robustqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
